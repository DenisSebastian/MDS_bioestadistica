[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Curso: Bioestadística 2022",
    "section": "",
    "text": "Este libro que contiene los apuntes personales del curso de Bioestadística. Master of Science, de la Universidad Adolfo Ibañez. Chile 2022.\nPorfesor Titular: Prof. Javier Lopatin | javier.lopatin@uai.cl\nPrincipalmente para los procesos análisis de datos y visualización de resultados se usará R Project\n\n\n\nIntroducción al lenguaje de programación R: Instalación del programa R y RStudio. Lectura de archivos externos. Gráficos y estadística descriptiva. Tipos básicos de herramientas estadísticas.\nRegresión lineal y no lineal: Correlación, estimación y predicción. Intervalos de confianza. Diagnósticos y Análisis de residuos. Calibración\nAnálisis multivariante: Métodos de reducción de dimensionalidad en los datos. PCA. Análisis de Clusters.\nAnálisis de varianza de un factor (one-way ANOVA): Fundamentos, métricas y ejemplos de uso.\nAnálisis de varianza de dos o más factores (two-way ANOVA): Diferencia con ANOVA de un factor. Ejemplos de uso.\nAnálisis de covarianza (ANCOVA): Fundamentos, métricas y ejemplos de uso.\nDiseños de muestreo: Muestreo aleatorio simple, estratificado, y sistemático. Muestreo de dos etapas. Determinación del tamaño de la muestra.\nEstadística no paramétrica: Introducción a la estadística no paramétrica. Cuando utilizarla. Principio de parsimonia.\nSeries de tiempo: Ajuste estacional, media móvil, suavizado exponencial, extrapolación, predicción lineal, Estimación de tendencias, estacionalidad y modelado ARIMA."
  },
  {
    "objectID": "fundamentos.html",
    "href": "fundamentos.html",
    "title": "2  Fundamendos Básicos",
    "section": "",
    "text": "Estadística:\n\nLa estadística es la ciencia de los datos. Consiste en recoger, clasificar, resumir, organizar analizar e interpretar la información numérica. (McClave and Sincich 2003)\n\n\n\n\nLa estadística es el análisis y la interpretación de los datos con vistas a una evaluación objetiva de la fiabilidad de las conclusiones basadas en los datos. (Zar 1999)\n\n\n\n\nLa capacidad de utilizar el pensamiento racional para interpretar el significado de los datos\nEsta capacidad puede ayudarle a tomar decisiones inteligentes, inferencias y generalizaciones\n(McClave and Sincich 2003)\n\n\n\n\nEl objetivo principal del análisis estadístico es inferir los parámetros de la población a partir de una muestra.\n\nPalabra “Parámetro” se refiere a las Poblaciones, e.j., µ, σ (i.e., letras Griegas)\nPalabra “Estadígrafo” (Statistics) se refiere a las Muestras, e.j., X, Sx (i.e., letras latinas)\n\nEstadígrafos pueden variar de muestra a muestra.\nQueremos que estos Estadígrafos sean buenas representaciones de los parámetros de la población.\n\n\n\n\n\nParámetros en estadística (población y muestra)\n\n\n\n\n\nDos tipos principales de estadística:\n\n\nDescriptiva\n\nUtiliza métodos numéricos y gráficos para buscar patrones en un conjunto de datos, para resumir la información revelada en un conjunto de datos y presentar esa información en una forma conveniente.\n\n\n\n\n\nInferencial\n\nEs una estimación, predicción o alguna otra generalización sobre una población basada en la información contenida en una muestra.\n\n\n\n(McClave and Sincich 2003)"
  },
  {
    "objectID": "fundamentos.html#e-descriptiva",
    "href": "fundamentos.html#e-descriptiva",
    "title": "2  Fundamendos Básicos",
    "section": "2.2 Estadística Descriptiva",
    "text": "2.2 Estadística Descriptiva\n\n\n\nEjemplos Estadísticas descriptiva con datos\n\n\n\n\n\nRepresentación de tipos de datos\n\n\n\n2.2.1 Muestras reperesentativas\n\nContiene características similares o típicas de la población.\nLa forma más común de obtener muestras representativas es mediante selección aleatoria.\nUna muestra aleatoria garantiza que cada subconjunto de tamaño fijo de la población tiene la misma probabilidad de ser incluido en la muestra\n\n\n\n2.2.2 Consideraciones de la Estadísticas Descriptiva\n\nDefinir la población o muestra de interés\nDefinir las variables o características de la población que se van a investigar.\nDefinir el tipo de estadística descriptiva a usar: gráficos, tablas, valores de resumen\nIdentificar patrones en los datos.\n\n(McClave and Sincich 2003)"
  },
  {
    "objectID": "fundamentos.html#m-tendencia-central",
    "href": "fundamentos.html#m-tendencia-central",
    "title": "2  Fundamendos Básicos",
    "section": "2.3 Medidas de Tendencias Central",
    "text": "2.3 Medidas de Tendencias Central\n\n\n\nMedidas de tendencia central\n\n\n\n\n\nDistribuciones Normales y Sesgadas\n\n\n(Zar 1999)\n\n2.3.1 Media Aritmética\n\nDefinición:\n\nLa media aritmética es un concepto matemático usado en estadística. También llamada promedio o simplemente media, se obtiene con la suma de un conjunto de valores dividida entre el número total de sumandos. Detalles en Wikipedia.\n\n\n\n\\mu=\\frac{\\displaystyle\\sum_{i=1}^{N}X_i}{N}\n\n\nX_i: cada observación de la población/muestra\nN: tamaño de la población o muestra\n\nEjemplo de Media Aritmétrica con R\nMuestra de 24 registros de población de mariposas cuyo valor corresponde a el largo en cm.\n\nValoresSumaMediaAlternativa MediaHistograma\n\n\n\nW <- c(3.3,3.5,3.6,3.6,3.7,3.8,\n       3.8,3.8,3.9,3.9,3.9,4.0,\n       4.0,4.0,4.0,4.1,4.1,4.1,\n       4.2,4.2,4.3,4.3,4.4,4.5)\n\n\n\n\n\\displaystyle\\sum_{i=1}^{N}X_i=95cm\n\n\nsum(W)\n\n[1] 95\n\n\n\n\n\n\\mu=\\frac{\\displaystyle\\sum_{i=1}^{N}X_i}{N}=3.96 cm\n\n\nmean(W)\n\n[1] 3.958333\n\n\n\n\n\nma <-  function(vector){\n  N = length(vector)\n  ma = sum(vector)/N\n  return(ma)\n}\nma(W)\n\n[1] 3.958333\n\n\n\n\n\nhist(W, col = \"gray97\")\nabline(v=mean(W), col=\"red\")\n\n\n\n\n\n\n\n\n\n2.3.2 Mediana\n\nDefinición:\n\nEn el ámbito de la estadística, la mediana (del latín medianus ‘del medio’) representa el valor de la variable de posición central en un conjunto de datos ordenados. Se le denota mediana, si la serie tiene un número par de puntuaciones, la mediana es la media entre las dos puntuaciones centrales.\n\n\nDetalles en wikipedia\n\nEs el valor del medio de un set de datos ordenados.\nSe puede entender también como el valor donde está el 50% de los datos.\n\n\n\n\nEncontrar la mediana\n\n\n\nsort(W)\n\n [1] 3.3 3.5 3.6 3.6 3.7 3.8 3.8 3.8 3.9 3.9 3.9 4.0 4.0 4.0 4.0 4.1 4.1 4.1 4.2\n[20] 4.2 4.3 4.3 4.4 4.5\n\n\n\nmedian(W)\n\n[1] 4\n\n\n\n\n2.3.3 Moda\n\nDefinición:\n\nEn la estadística, la moda es el valor que aparece con mayor frecuencia en un conjunto de datos. Esto va en forma de una columna cuando encontremos dos modas, es decir, dos datos que tengan la misma frecuencia absoluta máxima. Una distribución trimodal de los datos es en la que encontramos tres modas. En el caso de la distribución uniforme discreta, cuando todos los datos tienen una misma frecuencia, se puede definir las modas como indicado, pero estos lores no tienen utilidad. Por eso algunos matemáticos califican esta distribución como “sin moda”.\n\n\nDetalles en wikipedia\nValor más frecuente en un set de datos.\n\ntable(W)\n\nW\n3.3 3.5 3.6 3.7 3.8 3.9   4 4.1 4.2 4.3 4.4 4.5 \n  1   1   2   1   3   3   4   3   2   2   1   1 \n\n\nEn R no existe una función específica pero se puede crear una:\n\nmoda <- function(x) {\n  uniqv <- unique(x)\n  uniqv[which.max(tabulate(match(x, uniqv)))]\n}\nmoda(W)\n\n[1] 4"
  },
  {
    "objectID": "fundamentos.html#medidas-de-dispersión-y-variabilidad",
    "href": "fundamentos.html#medidas-de-dispersión-y-variabilidad",
    "title": "2  Fundamendos Básicos",
    "section": "2.4 Medidas de Dispersión y Variabilidad",
    "text": "2.4 Medidas de Dispersión y Variabilidad\n\n2.4.1 Rango\n\n\nDefinición:\n\nDiferencia entre el valor más alto y más bajo de la muestra\n\n\n\n\n\n\n\n\nWarning\n\n\n\nRango de la muestra subestima el rango de la población (problema con los extremos)\n\n\nEj., botánicos lo usan para medir las dimensiones de hojas y flores.\n\n\n\nRango\n\n\n\nCálculo Directo RLa función rango()\n\n\n\nmax(W) - min(W)\n\n[1] 1.2\n\n\n\n\nRetorna los valores mínimos y máximos de la muestra\n\nrange(W) \n\n[1] 3.3 4.5\n\n\n\n\n\n\n\n2.4.2 Cuartiles, Cuantiles y Percentiles\n\n\nCuartiles:\n\nDividen la población en 4 partes iguales, describiendo los valores acumulados al 0%, 25%, 50%, 75% y 100% (steps de 25%).\n\nCuantiles y Percentiles:\n\nDescriben lo mismo, pero no necesariamente se divide la muestra en 4 partes. Ej., se puede dividir en 10: 0%, 10%, 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, 100%\n\n\n\n\n\n\n\n\nCuartiles en RAjuste de los porcentajes\n\n\n\nquantile(W)\n\n   0%   25%   50%   75%  100% \n3.300 3.800 4.000 4.125 4.500 \n\n\n\n\n\nquantile(W, probs = c(0.05, 0.25, 0.5, 0.75, 0.95)) #\n\n   5%   25%   50%   75%   95% \n3.515 3.800 4.000 4.125 4.385 \n\n\n\n\n\n\n\n2.4.3 Rango Intercuartil (IQR)\n\nDistancia entre Q1 y Q3, el primer y segundo cuartil (25% y 75%).\nMás robusto que el rango normal\nNo afectan los outliars.\n\n\nIQR = Q3 - Q1\n\nRango intercuantil IQR = 75% - 25%\n\n\n\n\n\n\nCáculo en RAlternativa\n\n\n\nquantile(W, 0.75) - quantile(W, 0.25)\n\n  75% \n0.325 \n\n\n\n\n\nquantile(W)[4] - quantile(W)[2]\n\n  75% \n0.325 \n\n\n\n\n\n\n\n2.4.4 Varianza\n\nSuma de los cuadrados (SS) de las desviaciones de la media.\nDescribe la dispersión media en torno al valor medio\n\nDatos Población Total\n\n\\sigma^2 = \\frac{\\displaystyle\\sum_{i=1}^{n}(X_i - \\mu)^2} {N}\n\nDatos Muestrales\n\nS^2 = \\frac{\\displaystyle\\sum_{i=1}^{n}(X_i - \\bar{X})^2} {n-1}\n\n\nWikipedia: (“Varianza” 2022)\nVideo: (Matemáticas profe Alex 2017)\n\n\nvar(W)\n\n[1] 0.08514493\n\n\n\n\n2.4.5 Desviación Estándar\nEs una medida que se utiliza para cuantificar la variación o la dispersión de un conjunto de datos numéricos.\nUna desviación estándar baja indica que la mayor parte de los datos de una muestra tienden a estar agrupados cerca de su media (también denominada el valor esperado), mientras que una desviación estándar alta indica que los datos se extienden sobre un rango de valores más amplio.\n\nDesviación Estándar paraDatos Población Total\n\n\\sigma = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^{n}(X_i - \\mu)^2} {N}}\n\nDesviación Estándar para Datos Muestrales:\n\nS = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^{n}(X_i - \\bar{X})^2} {n-1}}\n\n\nValoresFórmulaFunción sd()GráficoCod_Graph\n\n\n\nW <- c(3.3,3.5,3.6,3.6,3.7,3.8,\n       3.8,3.8,3.9,3.9,3.9,4.0,\n       4.0,4.0,4.0,4.1,4.1,4.1,\n       4.2,4.2,4.3,4.3,4.4,4.5)\n\n\n\n\n\\sigma = \\sqrt{\\frac{\\displaystyle\\sum_{i=1}^{n}(X_i - \\mu)^2} {N}}\n\n\n\n\nsd(W)\n\n[1] 0.291796\n\n\n\n\n\n\n\n\n\n\n\n\n# graficar la +- 1 DE y +- 2 DE en un histograma?\nhist(W)\nabline(v=mean(W), col='red')\n\n## +- 1 DE ---------------------------------------------------------------\nabline(v=mean(W)+sd(W), col='blue')\nabline(v=mean(W)-sd(W), col='blue')\n\n## +- 2 DE ---------------------------------------------------------------\nabline(v=mean(W)+sd(W)*2, col='green')\nabline(v=mean(W)-sd(W)*2, col='green')\n\n\n\n\n\n\n2.4.6 Coeficiente de Variación\n\nCómo la desviación estándar, pero normalizado a un porcentaje.\nSirve para comparar la variación entre datos de distintas poblaciones/muestras!\n\nV=\\frac{s}{\\bar{X}}\n\nsd(W) / mean(W) * 100\n\n[1] 7.371689\n\n\n\n\n2.4.7 índices de Diversidad\n\nEn el caso de los datos de escala nominal, no existe una media o una mediana que sirva de referencia para hablar de la dispersión\nPodemos invocar el concepto de diversidad, la distribución de las observaciones entre las categorías\nObservaciones distribuidas uniformemente en las categorías tienen Diversidad alta, mientras que observaciones que ocurren en pocas clases tiene Div. baja.\n\n\n2.4.7.1 Shannon-Wiener diversity index\n\nH'= \\sum_{i=1}^k {p_i \\ log \\ p_i}\n\n\nK = número de clases\nP_i proporción de obs. de la clases i\n\n\n\n2.4.7.2 Shannon-Wiener evenness index (índice de uniformidad)\n\nJ' = \\frac{H'}{H_{max}} \\qquad H_{max}= log\\ k\n\n\n\n2.4.7.3 Datos con una dimensión 1D\n\nDataFormulaCódigoResultado\n\n\n\n\n\n\n \n  \n    especies \n    frecuencia \n  \n \n\n  \n    a \n    44 \n  \n  \n    b \n    3 \n  \n  \n    c \n    28 \n  \n  \n    d \n    12 \n  \n  \n    e \n    2 \n  \n  \n    f \n    8 \n  \n\n\n\n\n\n\n\n\nH'= \\sum_{i=1}^k {p_i \\ log \\ p_i}\n\n\n\n\nespecies   <- c('a','b','c','d','e','f')\nfrecuencia <- c(44,3,28,12,2,8)\n\n# funcion manual para vector de 1D\ndiversidad <- function(x){\n  x <- x/(total <- sum(x)) # Proporcion de cada especie\n  x <- -x * log(x, exp(1)) \n  H <- sum(x, na.rm = TRUE)\n  H\n}\n\n\n\n\ndiversidad(frecuencia)\n\n[1] 1.369117\n\n\n\n\n\nEjemplo en R de ídice de diversidad con la función diversity() de la librería vegan.\n\nDataCódigo\n\n\n\n\n\n\n \n  \n    especies \n    frecuencia \n  \n \n\n  \n    a \n    44 \n  \n  \n    b \n    3 \n  \n  \n    c \n    28 \n  \n  \n    d \n    12 \n  \n  \n    e \n    2 \n  \n  \n    f \n    8 \n  \n\n\n\n\n\n\n\n\nlibrary(vegan)\ndiversity(frecuencia) \n\n[1] 1.369117\n\n\n\n\n\n\n\n2.4.7.4 Datos con más de una dimensión\nLectura de datos\nBarro Colorado Island Tree Counts: Tree counts in 1-hectare plots in the Barro Colorado Island and associated site information.\n\ndata(BCI)\ndata(BCI.env)\n\nCálculo de BCI\n\n#### Diversity  ---------------------------------------------------------\ndiv <- diversity(BCI)\ndiv\n\n       1        2        3        4        5        6        7        8 \n4.018412 3.848471 3.814060 3.976563 3.969940 3.776575 3.836811 3.908381 \n       9       10       11       12       13       14       15       16 \n3.761331 3.889803 3.859814 3.698414 3.982373 4.017494 3.956635 3.916821 \n      17       18       19       20       21       22       23       24 \n3.736897 3.944985 4.013094 4.077327 3.969925 3.755413 4.062575 3.979427 \n      25       26       27       28       29       30       31       32 \n4.074718 3.947749 3.980281 3.693896 3.688721 3.851598 3.724967 3.784873 \n      33       34       35       36       37       38       39       40 \n3.740392 3.821669 2.641859 3.846109 3.791703 3.516082 3.530494 3.234849 \n      41       42       43       44       45       46       47       48 \n4.052495 3.966614 3.736254 3.705016 3.609518 3.810489 3.920918 3.913725 \n      49       50 \n3.778851 3.906616 \n\n\nAnalizar gradientes entre diversidad y variables\n\n# analizar gradientes entre diversidad y variables\nplot(x = div, y = BCI.env$Precipitation)\nabline(h=2530, col=\"red\")\n\n\n\n\n\nplot(x = div, y = BCI.env$Habitat)\n\n\n\n\n\n\n\n\nMatemáticas profe Alex. 2017. “Varianza y Desviación Estándar | Introducción,” June. https://www.youtube.com/watch?v=oZRaDwnpXkY.\n\n\nMcClave, J. T., and T. Sincich. 2003. Statistics. Prentice Hall.\n\n\n“Varianza.” 2022. https://es.wikipedia.org/w/index.php?title=Varianza&oldid=144468342.\n\n\nZar, Jerrold H. 1999. Biostatistical Analysis. Pearson Education India."
  },
  {
    "objectID": "d_prob.html",
    "href": "d_prob.html",
    "title": "3  Dist. de Probabilidad",
    "section": "",
    "text": "Espacio muestral: El conjunto de todos los posibles resultados de un experimento aleatorio\nDiscreto: Si cada resultado puede ponerse en correspondencia uno a uno con enteros positivos\nContinuo: Si sus resultados consisten de un intervalo de números reales\n\n(Canavos 1988)"
  },
  {
    "objectID": "d_prob.html#distribución-de-probabilidad-discreta",
    "href": "d_prob.html#distribución-de-probabilidad-discreta",
    "title": "3  Dist. de Probabilidad",
    "section": "3.2 Distribución de Probabilidad Discreta",
    "text": "3.2 Distribución de Probabilidad Discreta\nSi X es una variable aleatoria;\n\nSe llamará a p(x) = P(X=x) función de probabilidad de la variable aleatoria X, si satisface las siguientes propiedades:\n\n\np(x)\\geq 0, \\forall \\ x\\in X\\\\\n\\Sigma_x p(x)=1\n\nLa función de distribución acumulativa de la variable aleatoria X, es la probabilidad de que X sea menor o igual a un valor específico de x y está dada por:\n\nF(x)\\equiv P(X\\leq x) = \\sum_{x_i\\leq x}p(x_i)\n\nPropiedades:\n\n\n0\\leq F(x)\\leq, \\forall x\n\nEntre 0 y 1\n\n\n\n\nF(x_i)\\geq F(x_i) si x_i\\geq x_j\n\nMientras más grande el número X, más probabilidad acumulada\n\n\n\n\nP(X>x)= 1-F(x)\n\nLa P acumulada de x es igual a 1 - la P del número.\n\n\n(Canavos 1988)"
  },
  {
    "objectID": "d_prob.html#distribución-normal",
    "href": "d_prob.html#distribución-normal",
    "title": "3  Dist. de Probabilidad",
    "section": "3.3 Distribución Normal",
    "text": "3.3 Distribución Normal\nf(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n\n3.3.1 Gráficar en la Distribución Normal\nDiferentes alternativas\n\nAlternative 1: doing the math ourselvesAlternative 2: using dnorm()alternative 3: using cruve()\n\n\n\nx = seq(-4, 4, length = 200)\ny = 1 / sqrt(2 * pi) * exp(-x ^ 2 / 2)\n\nplot(x, y, type = \"l\", lwd = 4, col = \"red\", las = 1)\nabline(v = mean(x), col=\"gray60\")\n\n\n\n\n\n\n\nx = seq(-4,4,length=200)\ny = dnorm(x)\nplot(x, y, type = \"l\", lwd = 4, col = \"blue\", las = 1) # probar distintos argumentos\n\n\n\n#https://www.learnbyexample.org/r-plot-function/\n\n\n\n\ncurve(dnorm(x), -4, 4, col='green', ylab='y', lwd=4, las=1)\n\n\n\n\n\n\n\ndiferentes media \\mu y misma desviación estandar \\sigma\n\ndiferentes \\mu y misma \\sigmaDiferente \\sigma ymismo \\mu\n\n\n\ncurve(dnorm(x, mean=0, sd=1), -4, 4, col='blue', ylab='f(x)', lwd=2, las=1)\ncurve(dnorm(x, mean=1, sd=1), -4, 4, col='red', lwd=2, add=TRUE)\ncurve(dnorm(x, mean=2, sd=1), -4, 4, col='green', lwd=2, add=TRUE)\nlegend('topleft', legend=c('mean=0; sd=1','mean=1; sd=1','mean=2; sd=1'),\n       lwd=2, col=c('blue','red','green'), bty='n')\n\n\n\n\n\n\n\ncurve(dnorm(x, mean=0, sd=1), -4, 4, col='blue', ylab='f(x)', lwd=2, las=1)\ncurve(dnorm(x, mean=0, sd=1.5), -4, 4, col='red', lwd=2, add=TRUE)\ncurve(dnorm(x, mean=0, sd=2), -4, 4, col='green', lwd=2, add=TRUE)\n#abline(v = 0, lty = 2, col = 'gray')\nlegend('topleft', legend=c('mean=0; sd=1','mean=0; sd=1.5','mean=0; sd=2'),\n       lwd=2, col=c('blue','red','green'), bty='n')\n\n\n\n\n\n\n\nDistribución Acumulada\n\nprob <- pnorm(1, mean=0, sd=1)\nprob # percentage\n\n[1] 0.8413447\n\nxmin <- -4\nxmax <- 1\n\nx = seq(xmin, xmax, length=200)\ny = dnorm(x)\n\ncurve(dnorm(x), -4, 4, col = 'red', ylab = 'y', lwd = 2, las = 1, \n      main = 'Distribucion')\n\npolygon(c(xmin, x, xmax), c(0, y, 0), col = \"gray\")\ntext(0, 0.1, round(prob, 2), col = \"Red\")\n\n\n\n\n\nmean +- 1 sd = 68% measurementsprobability of having a value over 1.4?\n\n\nP(-1 <= x <= 1)\n\nprob <- pnorm(1) - pnorm(-1)\nprob\n\n[1] 0.6826895\n\nxmin <- -1\nxmax <- 1\n\nx = seq(xmin, xmax, length = 200)\ny = dnorm(x)\n\ncurve(dnorm(x), -4, 4, col = 'red', ylab='y', lwd = 2, las = 1)\npolygon(c(xmin, x, xmax), c(0, y, 0), col = \"gray\")\ntext(0, 0.1, round(prob, 3), col = \"Red\")\n\n\n\n\n\n\nP(x >= 1.4)\n\nprob <- 1-pnorm(1.4)\nprob\n\n[1] 0.08075666\n\nxmin <- 1.4\nxmax <- 4\n\nx = seq(xmin, xmax, length = 200)\ny = dnorm(x)\n\ncurve(dnorm(x), -4, 4, col = 'red', ylab = 'y', lwd = 2, las = 1)\npolygon(c(xmin, x, xmax), c(0, y, 0), col = \"gray\")\ntext(1.8, 0.03, round(prob, 3), col = \"Red\")"
  },
  {
    "objectID": "d_prob.html#estadísgrafos",
    "href": "d_prob.html#estadísgrafos",
    "title": "3  Dist. de Probabilidad",
    "section": "3.4 Estadísgrafos",
    "text": "3.4 Estadísgrafos\nQuantile–quantile plot (Q-Q plot): Gráfico traza las muestras clasificadas de nuestra distribución contra un número similar de cuantiles clasificados tomados de una distribución normal. Si la muestra está distribuida normalmente, la línea será recta. Las desviaciones de la normalidad aparecen como varios tipos de no linealidad (por ejemplo, formas de S o formas de plátano). Las funciones que necesita son qqnorm y qqline (gráfico de cuantiles contra una distribución normal):\n\n\nholaa\n\ndsds"
  },
  {
    "objectID": "d_prob.html#tarea",
    "href": "d_prob.html#tarea",
    "title": "3  Dist. de Probabilidad",
    "section": "3.5 Tarea",
    "text": "3.5 Tarea\n\n3.5.1 Usando la distribucion de largos de alas de mariposa que usamos en clases:\n\nW <- c(3.3,3.5,3.6,3.6,3.7,3.8,\n       3.8,3.8,3.9,3.9,3.9,4.0,\n       4.0,4.0,4.0,4.1,4.1,4.1,\n       4.2,4.2,4.3,4.3,4.4,4.5)\n\nhist(W)\n\n\n\n\n\n\n3.5.2 1. Asumiendo que la muestra es normal, haga un grafico de su distribucion de probabilidad\n\nx = W\n\ncurve(\n  dnorm(x = x, mean = mean(W), sd = sd(W)),\n  from = min(W),\n  to = max(W),\n  col = 'blue',\n  ylab = 'f(x)',\n  lwd = 2,\n  las = 1\n)\n\n\n\n\n\n\n3.5.3 2. Obtenga la probabilidad de que un valor sea menor a 4.2\n\ny <- dnorm(x,  mean = mean(W), sd = sd(W))\nprob <- dnorm(x = 4.2, mean = mean(W), sd = sd(W))\nprob # percentage\n\n[1] 0.9702521\n\nxmin <- min(W)\nxmax <- 4.2\n\ncurve(dnorm(x = x, mean = mean(W), sd = sd(W)),\n  from = min(W),\n  to = max(W),\n  col = 'red', ylab = 'y', lwd = 2, las = 1, \n  main = 'Distribucion')\n\nx = seq(xmin, xmax, length = 200)\ny <- dnorm(x,  mean = mean(W), sd = sd(W))\n\npolygon(c(xmin,x,xmax), c(0, y, 0), col = \"gray90\")\ntext(4.2, 0.5,round(prob, 2), col = \"Red\")\n\n\n\n\n\n\n3.5.4 3. Obtenga la probabilidad de que un valor sea menor a 4.1 y mayor a 3.8\n\ny <- dnorm(x,  mean = mean(W), sd = sd(W))\nprob <- dnorm(x = 4.2, mean = mean(W), sd = sd(W))\nprob # percentage\n\n[1] 0.9702521\n\nxmin <- min(W)\nxmax <- 4.2\n\ncurve(dnorm(x = x, mean = mean(W), sd = sd(W)),\n  from = min(W),\n  to = max(W),\n  col = 'red', ylab = 'y', lwd = 2, las = 1, \n  main = 'Distribucion')\n\nx = seq(xmin, xmax, length = 200)\ny <- dnorm(x,  mean = mean(W), sd = sd(W))\n\npolygon(c(xmin,x,xmax), c(0, y, 0), col = \"gray90\")\ntext(4.2, 0.5,round(prob, 2), col = \"Red\")\n\n\n\n\n\n\n3.5.5 4. Obtenga la probabilidad de que un valor sea mayor a 3.5\n\n# En Proceso\n\n\n\n\n\nCanavos, George C. 1988. “The Sensitivity of the One-Sample and Two-Sample Student t Statistics.” Computational Statistics & Data Analysis 6 (1): 39–46. https://doi.org/10.1016/0167-9473(88)90061-8."
  },
  {
    "objectID": "t_limite_central.html",
    "href": "t_limite_central.html",
    "title": "4  Teorema del Límite Central",
    "section": "",
    "text": "El teorema central del límite (TCL) es una teoría estadística que establece que, dada una muestra aleatoria suficientemente grande de la población, la distribución de las medias muestrales seguirá una distribución normal.\nSi se toman muestras repetidas de una población con varianza ﬁnita y se calculan sus promedios, entonces los promedios se distribuirán normalmente.\nEsto es verdad incluso cuando las muestras son tomadas de una distribución NO normal, siempre y cuando se tomen el suficiente número de muestras."
  },
  {
    "objectID": "t_limite_central.html#demostración-en-r",
    "href": "t_limite_central.html#demostración-en-r",
    "title": "4  Teorema del Límite Central",
    "section": "4.2 Demostración en R",
    "text": "4.2 Demostración en R\nCalculemos la media de cinco números aleatorios distribuidos uniformemente entre 0 y 10. La media será baja cuando obtengamos, ej., 2,3,1,2,1 y alta cuando obtengamos 9,8,9,6,8. Lo normal es que la media se acerque a 5. Hagamos esto 10.000 veces y observamos la distribución de las 10.000 medias. Los datos se distribuyen de forma rectangular (uniforme) en el intervalo de 0 a 10, por lo que la distribución de los datos brutos debería ser plana:\n\n# distribución de 10.000 números \n# aleatorios entre 0-10\nhist(runif(10000)*10, main=\"\")\n\n\n\nset.seed(1234) # fijar semilla para que el proceso aleatorio sea siempre igual\nA <- runif(10000)*10 # distribucion aleatoria uniforme\nhist(A)\n\n\n\n# media de la poblacion\nmediaA <- mean(A)\n\n¿Qué ocurre con la distribución de las medias muestrales, basada en la toma de sólo cinco números aleatorios uniformemente distribuidos?\n\n# creamos un vector numérico vacío de tamaño 10.000\nmeans <- numeric(10000)\n# llenamos el vector vacío con medias de 5 números aleatorios\nfor (i in 1:10000){\nmeans[i] <- mean(runif(5)*10)\n}\n\nhist(means,ylim=c(0,1600),main=\"\")\n\n\n\n\nSe ve bien, pero ¿cuan cerca está de una distribución normal?\n\nDibujar un distribución normal teórica usando X y S de la muestra.\nTest de normalidad.\n\n\nm <- mean(means)\ndesv <- sd(means)\n\nxv <- seq(0,10, 0.1)\nyv <-  dnorm(xv, mean = m, sd = desv)\n\nhist(means,ylim=c(0,1600),main=\"\")\nlines(xv, yv)\n\n\n\n\n\n4.2.1 Test de Normalidad\n\nqqnorm(means)\nqqline(means, lty=2)\n\n\n\nshapiro.test(sample(x = means, 5000))\n\n\n    Shapiro-Wilk normality test\n\ndata:  sample(x = means, 5000)\nW = 0.99905, p-value = 0.006669\n\n\nFunción para generar muestras de medias y visualiza el histograma\n\ngenerar_muestras_de_medias <- function(numero, muestras = 5){\n  means <- numeric(numero)\n  for (i in 1:numero){ means[i] <- mean(runif(muestras)*10) }\n  hist(means, main = paste('Distribucion', i, 'muestras'))\n}\n\nDiferencias entre medias muestrales con diferentes repeticiones\n\n# como se ven las diferencias entre medias muestrales con diferentes repeticiones?\npar(mfrow = c(2, 2))\nfor(i in c(10, 100, 1000, 100000)){\n  generar_muestras_de_medias(i)\n}\n\n\n\n\nDistribuciones en n\n\n\n\n\n\n\n4.2.2 Incrementar el número muestral\nCuando seleccionamos muestras de una distribución normal, la distribución de las medias muestrales de la muestra también tiene una forma “normal”.\nAumentar el tamaño muestral disminuye la dispersión.\n\nmeans <- numeric(5000)\nfor (i in 1:5000){ \n  means[i] <- mean(runif(5)*10) \n  }\n\nhist(means, col = 'lightblue', main = '')\n\n\n\n\nDistribución normal de 5000 muestras\n\n\n\n\n\nplot(density(means), main = '5.000 muestras')\nabline(v = 5, lty = 2, col = \"red\")\n\n\n\n\nvalor de la media de todas estas medias muestreales\n\n\n\nmean(means)\n\n[1] 4.977626\n\nsd(means)\n\n[1] 1.296756\n\n# media de la poblacion?\nmediaA\n\n[1] 5.003123\n\n\nEste comportamiento parece bastante razonable. Se esperaría una estimación más precisa de la media de la población original si tomamos la media de muestras de mayor tamaño.\n\n\n4.2.3 Distribución muestral de X\nMedia de la distribución muestral es = a las media de la población original\n\\mu_{\\bar{x}}=E(\\bar{x})=\\mu\nDesviación estándar de la distribución muestral igual:\n\\sigma_{\\bar{x}}=\\frac{\\sigma}{\\sqrt{n}} Este es el error estándar de la media.\n\n\n\nError estándar de la media\n\n\n\n\n4.2.4 Ajustar una curva normal\n\n# generar datos entre 0-1\nxv <- seq(0,10,0.1)\ndnorm(xv, mean = mean(means), sd = sd(means))\n\n  [1] 0.0001943350 0.0002605044 0.0003471333 0.0004598275 0.0006054955\n  [6] 0.0007925820 0.0010313233 0.0013340214 0.0017153317 0.0021925564\n [11] 0.0027859336 0.0035189095 0.0044183769 0.0055148635 0.0068426465\n [16] 0.0084397733 0.0103479613 0.0126123539 0.0152811078 0.0184047900\n [21] 0.0220355671 0.0262261737 0.0310286566 0.0364928978 0.0426649328\n [26] 0.0495850913 0.0572860011 0.0657905072 0.0751095740 0.0852402473\n [31] 0.0961637610 0.1078438832 0.1202255942 0.1332341894 0.1467748933\n [36] 0.1607330553 0.1749749848 0.1893494580 0.2036899049 0.2178172558\n [41] 0.2315433947 0.2446751414 0.2570186494 0.2683840879 0.2785904518\n [46] 0.2874703311 0.2948744665 0.3006759172 0.3047736796 0.3070956111\n [51] 0.3076005435 0.3062794967 0.3031559445 0.2982851205 0.2917523940\n [56] 0.2836707811 0.2741776932 0.2634310538 0.2516049365 0.2388848918\n [61] 0.2254631380 0.2115337888 0.1972882800 0.1829111425 0.1685762449\n [66] 0.1544436036 0.1406568265 0.1273412300 0.1146026351 0.1025268240\n [71] 0.0911796125 0.0806074751 0.0708386426 0.0618845848 0.0537417828\n [76] 0.0463936984 0.0398128491 0.0339629072 0.0288007499 0.0242784005\n [81] 0.0203448121 0.0169474606 0.0140337237 0.0115520364 0.0094528226\n [86] 0.0076892115 0.0062175523 0.0049977488 0.0039934361 0.0031720234\n [91] 0.0025046288 0.0019659284 0.0015339435 0.0011897846 0.0009173703\n [96] 0.0007031344 0.0005357342 0.0004057679 0.0003055086 0.0002286580\n[101] 0.0001701245\n\nhist(means, main = \"\", ylim = c(1,800)) \nyv <- dnorm(xv, mean = mean(means), sd = 1.28996)*2500\nlines(xv,yv)\n\n\n\n\n\n\n4.2.5 Test de Normalidad\n\nqqnorm(means)\nqqline(means, lty = 2, col = \"red\", lwd = 2)\n\n\n\n\nTest de Normalidad shapiro.test\n\n\n\nshapiro.test(means)\n\n\n    Shapiro-Wilk normality test\n\ndata:  means\nW = 0.99869, p-value = 0.0004339\n\n\np > 0.05 –> NO se rechaza H0 = es normal\np < 0.05 –> se rechaza H0 = NO es normal\nReferencias https://bookdown.org/dietrichson/metodos-cuantitativos/test-de-normalidad.html"
  },
  {
    "objectID": "t_limite_central.html#distribución-t-de-student",
    "href": "t_limite_central.html#distribución-t-de-student",
    "title": "4  Teorema del Límite Central",
    "section": "4.3 Distribución t de Student",
    "text": "4.3 Distribución t de Student\n\n\nt de Student\n\nEn probabilidad y estadística, la distribución t (de Student) es una distribución de probabilidad que surge del problema de estimar la media de una población normalmente distribuida cuando el tamaño de la muestra es pequeño y la desviación estándar poblacional es desconocida.\n\n\n\n\n\n\n\n\nNote\n\n\n\ncompletar formulación wiki\n\n\nSi el tamaño muestral (n) es muy largo (e.g., > 30), la distribución de t Student se aproxima a una distribución Normal.\n\npar(mfrow = c(1, 2))\ncurve(dt(x, df = 15), -4, 4, col = 'red', ylab = 'y', lwd = 2, las = 1, main = 'Dist. t Student')\ncurve(pt(x, df = 15), -4, 4, col = 'blue', ylab = 'y', lwd = 2, las = 1, main = 'Probabilidad t Student')\n\n\n\n\nlas distribuciones de probabilidad de la t student se generan a partir de dt y pt\n\n\n\n\nLa distribución Normal necesita de valores de \\mu y \\sigma. Acabamos de demostrar que no es posible estimar σ a partir de S:\n\nX es un estimador sin sesgo de \\mu\nS es un estimador sesgado de \\sigma\n\nt Student es una distribución que se describe con dos parámetros:\n\nX\nDF (deegrees of freedom), o grados de libertad\n\nTiene distinta forma según los grados de libertad (Degrees of freedom [DF])\nDF = ν = n - 1\nVariación de la distribucion de t Student con distintos DF\n\ndegf <- c(1, 3, 8, 30)\ncolors <- c(\"red\", \"blue\", \"darkgreen\", \"gold\", \"black\")\nlabels <- c(\"df = 1\", \"df = 3\", \"df = 8\", \"df = 30\", \"normal\")\n\n\npar(mfrow = c(1, 1))\ncurve(dnorm(x), -4, 4, lty=2, ylab='y', lwd=2, las=1)\n\n# plot t student's\nfor (i in 1:4){\n  curve(dt(x, degf[i]), lwd=2, col=colors[i], add=TRUE)\n}\n\nlegend(\"topright\", inset = .05, title = \"Distributions\", \n       labels, lwd = 2, lty = c(1, 1, 1, 1, 2), col = colors, bty = \"n\")\n\n\n\n\nLa distribución t se utiliza cuando:\n\nQueremos estimar la media de una población normalmente distribuida a partir de una muestra pequeña.\nTamaño de la muestra es inferior a 30 elementos, es decir, n < 30.\n\nNo se conoce la desviación típica o estándar de una población y tiene que ser estimada a partir de las observaciones de la muestra.\n ± 1 S\n67.3% No igual a Dist. Norm., pero Parecida\n\n4.3.1 Funciones para estimar el error\nEl error estándar: ::: {.cell}\nse <- function(x) { \n  sqrt(var(x)/length(x))\n}\n:::\nRango probable de valores para un Estadígrafo\nIntervalo de confianza = estadígrafo ± margen de error\n\nIntervConf <- function(x, alpha = 0.05) {\n  t.value <- qt((1-alpha), length(x)-1) # qt -> quantile function for t distrituion\n  standard.error <- se(x)\n  ci <- t.value*standard.error\n  cat(paste((1-alpha), \"Confidence Interval = \"), mean(x) - ci, \"to \", mean(x) + ci,\"\\n\") # concatenate and print\n  }\n\nIntervalo de confianza para estimar un parámetro de población, ej. la media:\nX ± t_{n-1} \\frac{S}{\\sqrt{n}}\nAplicación de la prueba\n\n# generamos datos de prueba\nx <- rnorm(150, 25, 3) # Recuerdan el orden de rnorm\nhist(x)\n\n\n\nmean(x)\n\n[1] 25.0514\n\n# intervalo deconfianza al 95%\nIntervConf(x)\n\n0.95 Confidence Interval =  24.63201 to  25.47079 \n\n# intervalo deconfianza al 99%\nIntervConf(x, alpha = 0.01)\n\n0.99 Confidence Interval =  24.45553 to  25.64727 \n\n\nNivel de confianza (alpha): Probabilidad que el intervalo de confianza contenga al parámetro de población\n\nEj., 95% de nivel de confianza\n\\alpha = 0.05; 1/20 veces de estar equivocado (Error tipo I)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "Referencias",
    "section": "",
    "text": "Canavos, George C. 1988. “The Sensitivity of the One-Sample and\nTwo-Sample Student t Statistics.” Computational Statistics\n& Data Analysis 6 (1): 39–46. https://doi.org/10.1016/0167-9473(88)90061-8.\n\n\nMatemáticas profe Alex. 2017. “Varianza y Desviación Estándar |\nIntroducción,” June. https://www.youtube.com/watch?v=oZRaDwnpXkY.\n\n\nMcClave, J. T., and T. Sincich. 2003. Statistics. Prentice\nHall.\n\n\n“Varianza.” 2022. https://es.wikipedia.org/w/index.php?title=Varianza&oldid=144468342.\n\n\nZar, Jerrold H. 1999. Biostatistical Analysis. Pearson\nEducation India."
  }
]